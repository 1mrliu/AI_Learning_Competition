# NLP的相关知识总结
## 1.NLP的数据特征表示问题
* 数据表示是机器学习的核心问题，在过去的Machine Learning阶段，大量兴起特征工程，人工设计大量的特征解决数据的有效表示问题。
而到了Deep Learning，想都别想，end-2-end，一步到位，hyper-parameter自动帮你选择寻找关键的特征参数。
## 2.语音、图片、文本在深度学习中表达的方式
* 在语音中，用音频频谱序列向量所构成的matrix作为前端输入喂给NN进行处理，good；在图像中，用图片的像素构成的matrix展平成vector后组成的vector序列喂给NN进行处理，good；那在自然语言处理中呢？噢你可能知道或者不知道，将每一个词用一个向量表示出来！想法是挺简单的，对，事实上就是这么简单，然而真有这么简单吗？可能没这么简单。
* 图片和声音的处理比较低级，图像和语音领域最基本的是信号的数据，可以通过距离度量，判断信号是否相似，在判断两幅图片是否相似的时候，只需通过观察图片本身就能给出答案。但是语言是一种高级抽象的思维信息表达工具，具有高度抽象的特征，文本是符号数据，两个词只要字面不同，就难以刻画他们之间的联系，即使是“话筒”和“麦克风”这两个同义词，从字面很难看出他们的意思相同，判断文本语言的时候，还需要更多的背景知识才能做出回答
* ***因此如何有效的表达出语言句子是决定NN能发挥出强大拟合计算能力的关键前提！***
## 3.NLP词的表示方法类型
* ***词的独热表示(one-hot representation)***  NLP中最直观，也是到目前为止最常用的词表示方式是  One-Hot Represention,这种方法把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中大多数为0，只有一个维度为1，这个维度就代表了当前的词。
举个例子：“话筒”表示为[0 0 0 0 1 0 0 0 ...],“麦克”表示为[0 0 0 0 0 0 0 0 0 1 0 0 0...] 每一个词都是1立0群的感觉。这种one-hot编码格式如果采用稀疏方式存储的话，会比较简洁，也就是给每一个词分配一个数字ID。例如，我们可以把话筒记为3，麦克记为8（假设从0开始）。编程实现的话，用Hash表给每一个词分配一个编号就可以了。这种简洁的表达方式配合上最大熵、SVM、CRF等算法就可以很好地完成NLP的各种主流任务。
* ***one-hot的缺点*** 1.向量的维度会随着句子的词的数量类型增大而不断增大。2.任意两个词之间都是孤立的，无法表示在语义层面上词与词之间的相关信息，这个问题对NLP处理影响是很大的。
* ***词的分布式表示（distributed representation）*** 传统的独热表示仅仅将词符号化，不包含任何的语义信息。如何将语义融合到词表示中？
最早的分布假说主要是：上下文相似的词，其语义也相似。词的语义由其上下文决定。
* 目前为止，基于分布假说的词表示方法，根据建模的不同，可以分为三类：
  * 1.基于矩阵的分布表示
  * 2.基于聚类的分布表示
  * 3.基于神经网络的分布表示
* 尽管这些不同的方法使用了不同的技术手段获取词表示，但是由于这些方法都是基于分布假说，他们的核心都是由两部分组成：
  * 1.选择一种方式描述上下文
  * 2.选择一种模型刻画某个词与其上下文之间的关系
## 4.NLP语言模型  
* 在详细介绍词的分布式表示之前，需要将NLP中的一个关键概念描述清楚： ***语言模型*** 。
***语言模型*** 包括文法语言模型和统计语言模型。一般我们指的是统计语言模型。之所以需要将语言模型摆在词表示方法前边，是因为后面的表示方法马上要用到这一概念。
* ***统计语言模型：*** 把语言（词的序列）看作一个随机事件，并赋予相应的概率来描述其属于某种语言集合的可能性。给定一个词汇集合V，对于V中的词构成的序列S={W1,...Wn}（属于V），统计语言模型富裕这个序列一个概率P(S)，来衡量S符合自然语言的语法和语义规则的置信度。
* 简单的说，语言模型就是计算一个句子的概率大小的这种模型。一个句子的打分概率越高，越说明他是更符合人说出来的语言的。
## 5.词嵌入的理解和总结
* 假设我们的词汇表含有 10,000 个单词，词汇表里有 a，aaron，orange，zulu，可能还有一个未知词标记<UNK>。我们要做的就是学习一个嵌入矩阵𝐸，它将是一个
300×10,000 的矩阵，如果你的词汇表里有 10,000 个，或者加上未知词就是 10,001 维。这个矩阵的各列代表的是词汇表中 10,000 个不同的单词所代表的不同向量
 
* 词嵌入中最主要的一件事就是我们的目标是学习一个嵌入矩阵𝐸。在某些场合中你将会随机地初始化矩阵𝐸，然后使用梯度下降法来学习这个 300×10,000 的矩阵中的各个参数，𝐸乘以这个 one-hot 向量（上图编号 1 所示）会得到嵌入向量。再多说一点，当我们写这个等式（上图编号2 所示）的时候，写出这些符号是很方便的，代表用矩阵𝐸乘以 one-hot向量𝑂𝑗。但当你动手实现时，用大量的矩阵和向量相乘来计算它，效率是很低下的，因为 onehot向量是一个维度非常高的向量，并且几乎所有元素都是 0，所以矩阵向量相乘效率太低，因为我们要乘以一大堆的 0。所以在实践中你会使用一个专门的函数来单独查找矩阵𝐸的某列，而不是用通常的矩阵乘法来做，但是在画示意图时（上图所示，即矩阵𝐸乘以 one-hot 向量示意图），这样写比较方便。但是例如在 Keras 中就有一个嵌入层，然后我们用这个嵌入层更有效地从嵌入矩阵中提取出你需要的列，而不是对矩阵进行很慢很复杂的乘法运算。

* 所以得出这种类比推理的结论的方法就是，当算法被问及 man 对 woman 相当于 king 对什么时，算法所做的就是计算𝑒man − 𝑒woman，然后找出一个向量也就是找出一个词，使得𝑒man − 𝑒woman≈ 𝑒king − 𝑒?，也就是说，当这个新词是 queen时，式子的左边会近似地等于右边。这种思想首先是被 Tomas Mikolov 和 Wen-tau Yih 还有Geoffrey Zweig 提出的，这是词嵌入领域影响力最为惊人和显著的成果之一，这种思想帮助了研究者们对词嵌入领域建立了更深刻的理解。
* ：余弦相似度 为了测量两个词的相似程度，我们需要一种方法来测量两个词的两
个 嵌 入 向 量 之 间 的 相 似 程 度 。 给 定 两 个向量 𝑢 和 𝑣 ， 余 弦 相 似 度 定 义可以自行查找。其中 𝑢*𝑣 是两个向量的点积（或内积），||u||2是向量𝑢的范数（或长度），并且 𝜃 是向量𝑢和𝑣之间的角度。这种相似性取决于角度在向量𝑢和𝑣之间。如果向量𝑢和𝑣非常相似，它 们 的 余 弦 相 似 性 将 接 近 1; 如 果 它 们 不 相 似 ， 则 余 弦 相 似 性 将 取 较 小 的 值。

* ***词嵌入和迁移学习：*** 用词嵌入做迁移学习的步骤。
   * 第一步，先从大量的文本集中学习词嵌入。一个非常大的文本集，或者可以下载网上预训练好的词嵌入模型，网上你可以找到不少，词嵌入模型并且都有许可。
   * 第二步，你可以用这些词嵌入模型把它迁移到你的新的只有少量标注训练集的任务中，比如说用这个 300 维的词嵌入来表示你的单词。这样做的一个好处就是你可      以用更低维度的特征向量代替原来的 10000 维的 one-hot 向量，现在你可以用一个 300 维更加紧凑的向量。尽管 one-hot 向量很快计算，而学到的用            于词嵌入的 300 维的向量会更加紧凑。
   * 第三步，当你在你新的任务上训练模型时，在你的命名实体识别任务上，只有少量的标记数据集上，你可以自己选择要不要继续微调，用新的数据调整词嵌入。实际      中，只有这个第二步中有很大的数据集你才会这样做，如果你标记的数据集不是很大，通常我不会在微调词嵌入上费力气。
## 案例：采集豆瓣网的电影评论信息
  * ***主要流程如下***
  * 1.分析豆瓣网的网页
  * 2.选择我们需要的数据进行抓取
  * 3.清理数据
  * 4.使用NLP中的WordCloud进行展示
  * 主要使用的库是 urllib BeautifulSoup jieba Wordcloud等
  ***代码----抓取数据到分词案例---***
  ![输出结果](https://github.com/1mrliu/AI_Learning_Competition/blob/master/NLP/result.png)
  
  
