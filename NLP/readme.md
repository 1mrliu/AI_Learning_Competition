# NLP的相关知识总结
## 1.NLP的数据特征表示问题
* 数据表示是机器学习的核心问题，在过去的Machine Learning阶段，大量兴起特征工程，人工设计大量的特征解决数据的有效表示问题。
而到了Deep Learning，想都别想，end-2-end，一步到位，hyper-parameter自动帮你选择寻找关键的特征参数。
## 2.语音、图片、文本在深度学习中表达的方式
* 在语音中，用音频频谱序列向量所构成的matrix作为前端输入喂给NN进行处理，good；在图像中，用图片的像素构成的matrix展平成vector后组成的vector序列喂给NN进行处理，good；那在自然语言处理中呢？噢你可能知道或者不知道，将每一个词用一个向量表示出来！想法是挺简单的，对，事实上就是这么简单，然而真有这么简单吗？可能没这么简单。
* 图片和声音的处理比较低级，图像和语音领域最基本的是信号的数据，可以通过距离度量，判断信号是否相似，在判断两幅图片是否相似的时候，只需通过观察图片本身就能给出答案。但是语言是一种高级抽象的思维信息表达工具，具有高度抽象的特征，文本是符号数据，两个词只要字面不同，就难以刻画他们之间的联系，即使是“话筒”和“麦克风”这两个同义词，从字面很难看出他们的意思相同，判断文本语言的时候，还需要更多的背景知识才能做出回答
* ***因此如何有效的表达出语言句子是决定NN能发挥出强大拟合计算能力的关键前提！***
## 3.NLP词的表示方法类型
* ***词的独热表示(one-hot representation)***  NLP中最直观，也是到目前为止最常用的词表示方式是  One-Hot Represention,这种方法把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中大多数为0，只有一个维度为1，这个维度就代表了当前的词。
举个例子：“话筒”表示为[0 0 0 0 1 0 0 0 ...],“麦克”表示为[0 0 0 0 0 0 0 0 0 1 0 0 0...] 每一个词都是1立0群的感觉。这种one-hot编码格式如果采用稀疏方式存储的话，会比较简洁，也就是给每一个词分配一个数字ID。例如，我们可以把话筒记为3，麦克记为8（假设从0开始）。编程实现的话，用Hash表给每一个词分配一个编号就可以了。这种简洁的表达方式配合上最大熵、SVM、CRF等算法就可以很好地完成NLP的各种主流任务。
* ***one-hot的缺点*** 1.向量的维度会随着句子的词的数量类型增大而不断增大。2.任意两个词之间都是孤立的，无法表示在语义层面上词与词之间的相关信息，这个问题对NLP处理影响是很大的。
* ***词的分布式表示（distributed representation）*** 传统的独热表示仅仅将词符号化，不包含任何的语义信息。如何将语义融合到词表示中？
最早的分布假说主要是：上下文相似的词，其语义也相似。词的语义由其上下文决定。
* 目前为止，基于分布假说的词表示方法，根据建模的不同，可以分为三类：
  * 1.基于矩阵的分布表示
  * 2.基于聚类的分布表示
  * 3.基于神经网络的分布表示
* 尽管这些不同的方法使用了不同的技术手段获取词表示，但是由于这些方法都是基于分布假说，他们的核心都是由两部分组成：
  * 1.选择一种方式描述上下文
  * 2.选择一种模型刻画某个词与其上下文之间的关系
## 4.NLP语言模型  
* 在详细介绍词的分布式表示之前，需要将NLP中的一个关键概念描述清楚： ***语言模型*** 。
***语言模型*** 包括文法语言模型和统计语言模型。一般我们指的是统计语言模型。之所以需要将语言模型摆在词表示方法前边，是因为后面的表示方法马上要用到这一概念。
* ***统计语言模型：***把语言（词的序列）看作一个随机事件，并赋予相应的概率来描述其属于某种语言集合的可能性。给定一个词汇集合V，对于V中的词构成的序列S={W1,...Wn}（属于V），统计语言模型富裕这个序列一个概率P(S)，来衡量S符合自然语言的语法和语义规则的置信度。
* 简单的说，语言模型就是计算一个句子的概率大小的这种模型。一个句子的打分概率越高，越说明他是更符合人说出来的语言的。
