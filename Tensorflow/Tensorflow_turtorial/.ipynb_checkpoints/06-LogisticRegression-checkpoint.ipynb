{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tempfile\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_num_checkpoint = 10\n",
    "num_classes = 2\n",
    "batch_size = 512\n",
    "num_epochs = 100\n",
    "\n",
    "initial_learning_rate = 0.001\n",
    "learning_rate_decay_factor = 0.95\n",
    "num_epochs_per_decay = 1\n",
    "\n",
    "is_training = False\n",
    "fine_tuning = False\n",
    "online_test = True\n",
    "allow_soft_placement = True\n",
    "log_device_placement = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "11623\n",
      "784\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", reshape=True, one_hot=False)\n",
    "\n",
    "data = {}\n",
    "\n",
    "data[\"train/image\"] = mnist.train.images\n",
    "data[\"train/label\"] = mnist.train.labels\n",
    "data[\"test/image\"] = mnist.test.images\n",
    "data[\"test/label\"] = mnist.test.labels\n",
    "\n",
    "def extract_samples_Fn(data):\n",
    "    index_list = []\n",
    "    for sample_index in range(data.shape[0]):\n",
    "        label = data[sample_index]\n",
    "        if label == 1 or label == 0:\n",
    "            index_list.append(sample_index)\n",
    "    return index_list\n",
    "\n",
    "index_list_train = extract_samples_Fn(data['train/label'])\n",
    "\n",
    "index_list_test = extract_samples_Fn(data['test/label'])\n",
    "\n",
    "data['train/image'] = mnist.train.images[index_list_train]\n",
    "data['train/label'] = mnist.train.labels[index_list_train]\n",
    "\n",
    "data['test/image'] = mnist.test.images[index_list_test]\n",
    "data['test/label'] = mnist.test.labels[index_list_test]\n",
    "\n",
    "dimensionality_train = data['train/image'].shape\n",
    "\n",
    "num_train_samples = dimensionality_train[0]\n",
    "num_features = dimensionality_train[1]\n",
    "print(num_train_samples)\n",
    "print(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss = 0.27497\n",
      "Epoch 2, Training Loss = 0.13461\n",
      "Epoch 3, Training Loss = 0.08467\n",
      "Epoch 4, Training Loss = 0.06259\n",
      "Epoch 5, Training Loss = 0.04997\n",
      "Epoch 6, Training Loss = 0.04163\n",
      "Epoch 7, Training Loss = 0.03570\n",
      "Epoch 8, Training Loss = 0.03129\n",
      "Epoch 9, Training Loss = 0.02788\n",
      "Epoch 10, Training Loss = 0.02516\n",
      "Epoch 11, Training Loss = 0.02296\n",
      "Epoch 12, Training Loss = 0.02115\n",
      "Epoch 13, Training Loss = 0.01963\n",
      "Epoch 14, Training Loss = 0.01834\n",
      "Epoch 15, Training Loss = 0.01723\n",
      "Epoch 16, Training Loss = 0.01628\n",
      "Epoch 17, Training Loss = 0.01544\n",
      "Epoch 18, Training Loss = 0.01471\n",
      "Epoch 19, Training Loss = 0.01406\n",
      "Epoch 20, Training Loss = 0.01348\n",
      "Epoch 21, Training Loss = 0.01296\n",
      "Epoch 22, Training Loss = 0.01250\n",
      "Epoch 23, Training Loss = 0.01208\n",
      "Epoch 24, Training Loss = 0.01170\n",
      "Epoch 25, Training Loss = 0.01135\n",
      "Epoch 26, Training Loss = 0.01103\n",
      "Epoch 27, Training Loss = 0.01074\n",
      "Epoch 28, Training Loss = 0.01048\n",
      "Epoch 29, Training Loss = 0.01023\n",
      "Epoch 30, Training Loss = 0.01001\n",
      "Epoch 31, Training Loss = 0.00980\n",
      "Epoch 32, Training Loss = 0.00961\n",
      "Epoch 33, Training Loss = 0.00943\n",
      "Epoch 34, Training Loss = 0.00926\n",
      "Epoch 35, Training Loss = 0.00910\n",
      "Epoch 36, Training Loss = 0.00896\n",
      "Epoch 37, Training Loss = 0.00882\n",
      "Epoch 38, Training Loss = 0.00869\n",
      "Epoch 39, Training Loss = 0.00858\n",
      "Epoch 40, Training Loss = 0.00846\n",
      "Epoch 41, Training Loss = 0.00836\n",
      "Epoch 42, Training Loss = 0.00826\n",
      "Epoch 43, Training Loss = 0.00817\n",
      "Epoch 44, Training Loss = 0.00808\n",
      "Epoch 45, Training Loss = 0.00800\n",
      "Epoch 46, Training Loss = 0.00792\n",
      "Epoch 47, Training Loss = 0.00784\n",
      "Epoch 48, Training Loss = 0.00777\n",
      "Epoch 49, Training Loss = 0.00771\n",
      "Epoch 50, Training Loss = 0.00764\n",
      "Epoch 51, Training Loss = 0.00759\n",
      "Epoch 52, Training Loss = 0.00753\n",
      "Epoch 53, Training Loss = 0.00748\n",
      "Epoch 54, Training Loss = 0.00743\n",
      "Epoch 55, Training Loss = 0.00738\n",
      "Epoch 56, Training Loss = 0.00733\n",
      "Epoch 57, Training Loss = 0.00729\n",
      "Epoch 58, Training Loss = 0.00725\n",
      "Epoch 59, Training Loss = 0.00721\n",
      "Epoch 60, Training Loss = 0.00717\n",
      "Epoch 61, Training Loss = 0.00713\n",
      "Epoch 62, Training Loss = 0.00710\n",
      "Epoch 63, Training Loss = 0.00707\n",
      "Epoch 64, Training Loss = 0.00704\n",
      "Epoch 65, Training Loss = 0.00701\n",
      "Epoch 66, Training Loss = 0.00698\n",
      "Epoch 67, Training Loss = 0.00695\n",
      "Epoch 68, Training Loss = 0.00693\n",
      "Epoch 69, Training Loss = 0.00690\n",
      "Epoch 70, Training Loss = 0.00688\n",
      "Epoch 71, Training Loss = 0.00686\n",
      "Epoch 72, Training Loss = 0.00683\n",
      "Epoch 73, Training Loss = 0.00681\n",
      "Epoch 74, Training Loss = 0.00679\n",
      "Epoch 75, Training Loss = 0.00678\n",
      "Epoch 76, Training Loss = 0.00676\n",
      "Epoch 77, Training Loss = 0.00674\n",
      "Epoch 78, Training Loss = 0.00673\n",
      "Epoch 79, Training Loss = 0.00671\n",
      "Epoch 80, Training Loss = 0.00669\n",
      "Epoch 81, Training Loss = 0.00668\n",
      "Epoch 82, Training Loss = 0.00667\n",
      "Epoch 83, Training Loss = 0.00665\n",
      "Epoch 84, Training Loss = 0.00664\n",
      "Epoch 85, Training Loss = 0.00663\n",
      "Epoch 86, Training Loss = 0.00662\n",
      "Epoch 87, Training Loss = 0.00661\n",
      "Epoch 88, Training Loss = 0.00660\n",
      "Epoch 89, Training Loss = 0.00659\n",
      "Epoch 90, Training Loss = 0.00658\n",
      "Epoch 91, Training Loss = 0.00657\n",
      "Epoch 92, Training Loss = 0.00656\n",
      "Epoch 93, Training Loss = 0.00655\n",
      "Epoch 94, Training Loss = 0.00654\n",
      "Epoch 95, Training Loss = 0.00653\n",
      "Epoch 96, Training Loss = 0.00653\n",
      "Epoch 97, Training Loss = 0.00652\n",
      "Epoch 98, Training Loss = 0.00651\n",
      "Epoch 99, Training Loss = 0.00651\n",
      "Epoch 100, Training Loss = 0.00650\n",
      "Final Test Accuracy is % 99.95\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # global step\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    # learning rate policy\n",
    "    decay_steps = int(num_train_samples / batch_size * num_epochs_per_decay)\n",
    "    \n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate,\n",
    "                                              global_step,\n",
    "                                              decay_steps,\n",
    "                                              learning_rate_decay_factor,\n",
    "                                              staircase=True,\n",
    "                                               name='eponential_decay_learning_rate'\n",
    "                                              )\n",
    "    # defining place holders\n",
    "    image_place = tf.placeholder(tf.float32, shape=([None, num_features]), name='image')\n",
    "    label_place = tf.placeholder(tf.int32, shape=([None,]),name='gt')\n",
    "    label_one_hot = tf.one_hot(label_place,depth=num_classes)\n",
    "    dropout_param = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Model + loss + Accuracy\n",
    "    # fully connected with two class and softmax \n",
    "    logits = tf.contrib.layers.fully_connected(inputs=image_place, num_outputs=num_classes,scope='fc')\n",
    "    \n",
    "    # loss\n",
    "    with tf.name_scope('loss'):\n",
    "        loss_tensor = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=label_one_hot))\n",
    "    \n",
    "    # accuracy  evaluate the model\n",
    "    prediction_correct = tf.equal(tf.argmax(logits,1), tf.argmax(label_one_hot,1))\n",
    "    \n",
    "    # accuracy calculation\n",
    "    accuracy = tf.reduce_mean(tf.cast(prediction_correct,tf.float32))\n",
    "    \n",
    "    # 优化器\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    # gradient update\n",
    "    with tf.name_scope('train_op'):\n",
    "        gradients_and_variables = optimizer.compute_gradients(loss_tensor)\n",
    "        train_op = optimizer.apply_gradients(gradients_and_variables,global_step=global_step)\n",
    "    \n",
    "    # Run the session\n",
    "    session_conf = tf.ConfigProto(\n",
    "        allow_soft_placement=allow_soft_placement,\n",
    "        log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(graph=graph,config=session_conf)\n",
    "    \n",
    "    with sess.as_default():\n",
    "        # saver op\n",
    "        saver = tf.train.Saver()\n",
    "        # initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # the prefix for checkpoint files\n",
    "        checkpoint_prefix = 'model'\n",
    "        \n",
    "        if fine_tuning:\n",
    "            saver.restore(sess, os.path.join(checkpoint_path,checkpoint_prefix))\n",
    "            print(\"Model restored for fine-tuning...\")\n",
    "        \n",
    "        # training  loop over batches\n",
    "        \n",
    "        test_accuracy = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            total_batch_training = int(data['train/image'].shape[0] / batch_size)\n",
    "            \n",
    "            for batch_num in range(total_batch_training):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = (batch_num + 1) * batch_size\n",
    "                \n",
    "                train_batch_data, train_batch_label = data['train/image'][start_index:end_index], data['train/label'][start_index:end_index]\n",
    "             \n",
    "                batch_loss, _, training_step = sess.run([loss_tensor,train_op,global_step],\n",
    "                                                   feed_dict={image_place:train_batch_data,\n",
    "                                                             label_place:train_batch_label,\n",
    "                                                             dropout_param:0.5})\n",
    "            print(\"Epoch \" + str(epoch + 1)+ \", Training Loss = \"+ \\\n",
    "                 \"{:.5f}\".format(batch_loss))\n",
    "        \n",
    "        test_accuracy = 100 * sess.run(accuracy, feed_dict={\n",
    "            image_place:data['test/image'],\n",
    "            label_place:data['test/label'],\n",
    "            dropout_param:1.\n",
    "        })\n",
    "        print(\"Final Test Accuracy is %% %.2f\" % test_accuracy)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
